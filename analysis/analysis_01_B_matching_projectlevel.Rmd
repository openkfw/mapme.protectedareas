---
title: "Matching"
author: "Melvin HL Wong"
date: "10 10 2022"
output: workflowr::wflow_html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_knit$set(root.dir = "~/shared/datalake/mapme.protectedareas")
```


```{r include=FALSE}
# clean workspace, set options
rm(list=ls())
options(scipen=999)

# get packages
lop <- c("dplyr", "plm", "stargazer", "tidyverse", "MatchIt", "glm", "optmatch", "cobalt", "gridExtra")
newp <- lop[!(lop %in% installed.packages()[,"Package"])]
if(length(newp)) install.packages(newp)
lapply(lop, require, character.only = TRUE)

```


## Get list of WDPA areas that first received KfW funds
```{r}
wdpa_kfw<-
  read_sf("~/shared/datalake/mapme.protectedareas/input/wdpa_kfw/wdpa_kfw_spatial_latinamerica_2021-02-01_supportedPAs_unique.gpkg")

# Load KfW finance data 
kfw_finance <- 
  read_csv("~/shared/datalake/mapme.protectedareas/input/kfw_finance/mapme.protectedareas_kfw-finance-2021-03-17.csv") %>% 
  filter(! is.na(bmz_nummer))

#Load wdpa bmz keys
keys_wdpaid_bmz <- read_csv("~/shared/datalake/mapme.protectedareas/output/matching/model_frames/keys_wdpaid_bmz.csv") %>% 
  rename("bmz_nummer" = "value")

# merge wdpa_kfw with keys
wdpa_kfw <- left_join(wdpa_kfw, keys_wdpaid_bmz,
                      by=c("WDPAID"))

#add kfw_finance data to wdpa_kfw
wdpa_kfw <- left_join(wdpa_kfw, kfw_finance, 
                      by=c("bmz_nummer"))

wdpa_year <- wdpa_kfw %>% 
  select(WDPAID, first_year) %>% 
  st_drop_geometry 
# get list of number of treatment years
wdpa_year <-   wdpa_year[order(wdpa_year$WDPAID, wdpa_year$first_year),] %>% 
  distinct()
n_occur <- data.frame(table(wdpa_year$WDPAID)) %>% 
  rename(WDPAID=Var1)

# create table with list of treatment years
wdpa_year_mult <- wdpa_year %>% 
  merge(.,n_occur) %>% 
  rename(year=first_year) 

# get the number of treatment (ordered)
wdpa_year_mult$Freq_order <- NA
for (i in unique(wdpa_year_mult$WDPAID)){
  print(i)
  wdpa_year_mult$Freq_order[which(wdpa_year_mult$WDPAID==i)] <- 1:max(wdpa_year_mult$Freq[which(wdpa_year_mult$WDPAID==i)])
}

# create variables indicating treatment years
wdpa_year_mult$first_year <- NA
wdpa_year_mult$first_year[which(wdpa_year_mult$Freq_order==1)] <- wdpa_year_mult$year[which(wdpa_year_mult$Freq_order==1)]
  
wdpa_year_mult$second_year <- NA
wdpa_year_mult$second_year[which(wdpa_year_mult$Freq_order==2)] <- wdpa_year_mult$year[which(wdpa_year_mult$Freq_order==2)]

wdpa_year_mult$third_year <- NA
wdpa_year_mult$third_year[which(wdpa_year_mult$Freq_order==3)] <- wdpa_year_mult$year[which(wdpa_year_mult$Freq_order==3)]

# collapse data so that there are unique WDPAIDs
wdpa_year_mult <- wdpa_year_mult %>% 
  group_by(WDPAID) %>% 
  summarise(
    first_year=mean(first_year, na.rm = T),
    second_year=mean(second_year, na.rm = T),
    third_year=mean(third_year, na.rm = T)
  ) %>% 
  mutate_all(~ifelse(is.nan(.), NA, .)) # replace all NaN with NA


# Load link between assetid and WDPA ID
keys_assetid_wdpa <- 
  read_sf("~/shared/datalake/mapme.protectedareas/processing/fishnet/honeycomb_5_sqkm_kfw.gpkg") %>% 
  select(poly_id, WDPAID) %>% 
  rename(".assetid" = "poly_id") %>% 
  st_drop_geometry()

# merge WDPA with treatment indicators and assetid
keys_assetid_wdpa <- keys_assetid_wdpa %>% 
  distinct(.assetid, WDPAID) %>%  
  merge(.,wdpa_year_mult) %>% 
  distinct(.assetid, .keep_all = T) # some .assetid share the same WDPAID and have different "first_years". That is because the different protected areas are close together and the cell overlaps both PA at the border. The duplicated entries are only a few but cause problems in merging data. Solution. Drop them. About 9% loss of cells still (18371/195267)


```


## Propensity Score Matching

```{r Propensity Score Matching (PSM)}
# Timer
start_time <- Sys.time()

#------- Select years and prepare table -------

# Select project matching frames. (no BMZ number started in these years 2018, 2020)

T_year <- list.files("/datadrive/datalake/mapme.protectedareas/output/matching/matching_frames/projects")

# drop BMZ Numbers before 2004 and BMZ number starting with 2098
T_year <- T_year[15:33]
for (i in T_year) {
  
  #------- Load matching frames -------
  #file <- paste0("matching_frame_",i)
  # df <- 
  #   read_csv(paste("./output/matching/matching_frames/matching_frame_",i,".csv", sep = "")) %>% 
  #   select(id, poly_id, treatment, everything())
  # 
  #  df <- df %>% 
  #   filter(! is.na(country),
  #          ! is.na(clay_content_10_cm))
  
  # Load data
  df <- read_rds(paste0("/datadrive/datalake/mapme.protectedareas/output/matching/matching_frames/projects/",i)) %>% merge(., keys_assetid_wdpa, by=".assetid", all.x=T)
  # we need this t to fix the time varying variables (especially fc_loss). The problem: Some .assetid are overlapping into other WDPA.Most likely because we include buffer zones that are entirely outside of the PA but may range into another PA. Solution: Look up the number of .assetid for each first_year year and chose the first_year with the most .assetid
first_year_num <- length(unique(df$first_year[!is.na(df$first_year)]))
    if (first_year_num ==1){ #if only one year in first year (without counting NA)
      t <- df %>% 
      select(first_year) %>% 
      na.omit() %>% 
      unique() %>% 
      as.numeric()
      
    }else{ # if mulitple years, only keep first_year with maximum number of .assetid
      # get number of first_years
      list_year <- df %>% 
      select(first_year) %>% 
      na.omit() %>% 
      unique() 
      # create empty variable to be filled out with frequency
      list_year$num_asset <- NA
      # Calculate frequency of first_years over .assetid
      for (x in 1:first_year_num) {
        temp <- df %>% 
        select(.assetid, first_year)%>% 
        na.omit() %>% 
        filter(first_year==list_year[x,1]) 
        list_year$num_asset[which(list_year$first_year==list_year[x,1])] <- dim(temp)[1]
      }
      # save first_year with maximum number of assetid as t
      t <- list_year$first_year[which(list_year$num_asset==max(list_year$num_asset))]
      
    }

    
  bmz <- gsub("matching_frame_", "", i) %>% 
    gsub(".rds", "", .)
  
  
  
  df <- df %>% 
    select(.assetid,
           treatment,
           ends_with(as.character(t)),
           traveltime_5k_110mio,
           traveltime_20k_110mio,
           terrain_ruggedness_index_mean,
           elevation_mean,
           soil_5_15cm_clay,
           NAME_0) %>% 
    rename_with(~str_remove(., paste0("_",t))) # drop year suffix in column name
  
  df <- df %>% 
    filter(! is.na(treecover),
           ! is.na(NAME_0),
           ! is.na(traveltime_5k_110mio),
           ! is.na(terrain_ruggedness_index_mean),
           ! is.na(elevation_mean),
           ! is.na(soil_5_15cm_clay))
   
  #------- NN PS matching w/ replacement and exact matching 'country'-------
  
#  tryCatch({
    
    # Get propensity scores
    glm_out1 <- glm(treatment ~ 
                      traveltime_5k_110mio + 
                      soil_5_15cm_clay + 
                      terrain_ruggedness_index_mean +
                      elevation_mean + 
                      as.factor(NAME_0) +
                      treecover +
                      loss_tn,
                    family = binomial(link = "probit"),
                    data = df)
      
    
    stargazer(glm_out1,
              summary=TRUE,
              type="text",
              title = paste0("Probit regression for matching frame ",bmz) #,
              # out = paste0("../../datalake/mapme.protectedareas/output/plots/propensity_score_matching/probit_summary_",i,".html")
              )
    
  
    m_out1 <- matchit(treatment ~ 
                        traveltime_5k_110mio + 
                        soil_5_15cm_clay + 
                        terrain_ruggedness_index_mean +
                        elevation_mean + 
                        as.factor(NAME_0) +
                        treecover +
                        loss_tn,
                      data = df,
                      method = "nearest",
                      replace = TRUE,
                      exact = ~ as.factor(NAME_0),
                      distance = "glm", 
                      discard = "both", # common support: drop units from both groups 
                      link = "probit")
    
    # Save matchit object
    write_rds(m_out1, paste0("/datadrive/datalake/mapme.protectedareas/output/matching/matchit_objects/PSM/tn_projects/m_psm_tn_", bmz, ".rds"))
    
    print(m_out1)
    print(summary(m_out1, un = FALSE))
    
    # Balance table
    bal_table <- bal.tab(m_out1, un = TRUE)
    print(bal_table)
    
    
    # Matched data
    m_data1 <- match.data(m_out1)
    
    ## Export data
    # write_csv(m_data1, 
    #       paste0("../../datalake/mapme.protectedareas/output/tabular/regression_input/PSM/tn_projects/ps_matched_data_", bmz, ".csv"))
    write_rds(m_data1, 
            paste0("/datadrive/datalake/mapme.protectedareas/output/tabular/regression_input/PSM/tn_projects/psm_tn_matched_data_", bmz, ".rds"))
  
#  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  
}
```

## Coarsened Exact Matching

```{r function for CEM}
# Build function for matching (CEM) 

custom_cem <- function(bmz, cutoffs_travel_time, cutoffs_loss_tn, cutoffs_elevation, cutoffs_tri, cutoffs_soil) {
  
  print(bmz)
  df <- 
    read_rds(paste0("/datadrive/datalake/mapme.protectedareas/output/matching/matching_frames/projects/matching_frame_", bmz, ".rds"))%>% merge(., keys_assetid_wdpa, by=".assetid", all.x=T)
  # we need this t to fix the time varying variables (especially fc_loss). The problem: Some .assetid are overlapping into other WDPA.Most likely because we include buffer zones that are entirely outside of the PA but may range into another PA. Solution: Look up the number of .assetid for each first_year year and chose the first_year with the most .assetid
first_year_num <- length(unique(df$first_year[!is.na(df$first_year)]))
    if (first_year_num ==1){ #if only one year in first year (without counting NA)
      year <- df %>% 
      select(first_year) %>% 
      na.omit() %>% 
      unique() %>% 
      as.numeric()
      
    }else{ # if mulitple years, only keep first_year with maximum number of .assetid
      # get number of first_years
      list_year <- df %>% 
      select(first_year) %>% 
      na.omit() %>% 
      unique() 
      # create empty variable to be filled out with frequency
      list_year$num_asset <- NA
      # Calculate frequency of first_years over .assetid
      for (x in 1:first_year_num) {
        temp <- df %>% 
        select(.assetid, first_year)%>% 
        na.omit() %>% 
        filter(first_year==list_year[x,1]) 
        list_year$num_asset[which(list_year$first_year==list_year[x,1])] <- dim(temp)[1]
      }
      # save first_year with maximum number of assetid as t
      year <- list_year$first_year[which(list_year$num_asset==max(list_year$num_asset))]
      
    }

  
  # Load data
  df <- 
    read_rds(paste0("/datadrive/datalake/mapme.protectedareas/output/matching/matching_frames/projects/matching_frame_", bmz, ".rds")) %>% 
    select(.assetid,
           treatment,
           ends_with(as.character(year)),
           traveltime_5k_110mio,
           traveltime_20k_110mio,
           terrain_ruggedness_index_mean,
           elevation_mean,
           soil_5_15cm_clay,
           NAME_0) %>% 
    rename_with(~str_remove(., paste0("_",year))) # drop year suffix in column name
  
  # Drop NA values
  df <- df %>% 
    filter(! is.na(treecover),
           ! is.na(NAME_0),
           ! is.na(traveltime_5k_110mio),
           ! is.na(terrain_ruggedness_index_mean),
           ! is.na(elevation_mean),
           ! is.na(soil_5_15cm_clay))
  
  # Create cutoffs list
  cutoffs_list <- c()
  
  # Add cutoff parameters
  cutoffs_list$traveltime_5k_110mio <-
    cutoffs_travel_time
  
  cutoffs_list$terrain_ruggedness_index_mean <- 
    cutoffs_tri
  
  cutoffs_list$elevation_mean <- 
    cutoffs_elevation
  
  cutoffs_list$loss_tn <- 
    cutoffs_loss_tn
  
  cutoffs_list$soil_5_15cm_clay <- 
    cutoffs_soil
  
  
  m_out2 <- matchit(treatment ~ 
                      traveltime_5k_110mio + 
                      soil_5_15cm_clay + 
                      terrain_ruggedness_index_mean +
                      elevation_mean + 
                      as.factor(NAME_0) +
                      treecover +
                      loss_tn,
                    data = df,
                    method = "cem",
                    cutpoints = cutoffs_list)
  
  # Save matchit object
  write_rds(m_out2, paste0("/datadrive/datalake/mapme.protectedareas/output/matching/matchit_objects/CEM/tn_projects/m_cem_tn_", bmz, ".rds"))
  
  # Matching summary
  print(m_out2)
  print(summary(m_out2, un = FALSE))
  
  # Balance table
  bal_table <- bal.tab(m_out2, un = TRUE)
  print(bal_table)
  
  # Matched data
  m_data2 <- match.data(m_out2) 
  
  ## Export data
  # write_csv(m_data2, 
  #           paste0("../../datalake/mapme.protectedareas/output/tabular/regression_input/CEM/tn_projects/ce_matched_data_", i, ".csv"))
  write_rds(m_data2, 
            paste0("/datadrive/datalake/mapme.protectedareas/output/tabular/regression_input/CEM/tn_projects/cem_tn_matched_data_", bmz, ".rds"))
  
}

```

### Reference cutoffs CEM

```{r reference cutoffs}
  # Add cutoff parameters
  # cutoffs_list$traveltime_5k_110mio <-
  # c(0,120,300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, max(df$traveltime_5k_110mio,na.rm = T)) # 0-2hrs, 2-6 hrs, >6 hrs
  # cutoffs_list$terrain_ruggedness_index_mean <-
  # c(0,5,10,max(df$terrain_ruggedness_index_mean,na.rm = T))
  # cutoffs_list$elevation_mean <-
  # c(0,500,1500,max(df$elevation_mean,na.rm = T))
  # cutoffs_list$loss_t3 <-
  # c(-1,0, 1, 10, 100, 200, 300, 400, 500, 600, 1000, max(df$loss_t3))
  # cutoffs_list$soil_5_15cm_clay <-
  # c(2.440179, 6.091119,  9.742059, 13.392999, 17.043938, 20.694878, 24.345818, 27.996758, 31.647698, 35.298637, 38.949577, 42.600517, 46.251457, 49.902397, 53.553337, 57.204276, 60.855216, 64.506156)
  # cutoffs_list$loss <- 
  #   c(0.0000,  509.4486, 1018.8973, 1528.3459, 2037.7946, 2547.2432, 3056.6918, 3566.1405, 4075.5891, 4585.0378, 5094.4864, 5603.9350, 6113.3837, 6622.8323, 7132.2810, 7641.7296, 8151.1782, 8660.6269)
```
 
## Matching frame 2004
 
```{r matching frame}
# Assign values to input parameters
cutoffs_travel_time <- c(0,120,300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 12015.71)
cutoffs_loss_tn <- c(-1,0, 1, 10, 100, 200, 300, 400, 500, 600, 1000)
cutoffs_elevation <- c(0,500,1500,6138.56)
cutoffs_tri <- c(0,5,10,182.1068)
cutoffs_soil <- c(2.440179, 6.091119,  9.742059, 13.392999, 17.043938, 20.694878, 24.345818, 27.996758, 31.647698, 35.298637, 38.949577, 42.600517, 46.251457, 49.902397, 53.553337, 57.204276, 60.855216, 64.506156)
# Call function
T_year <- list.files("/datadrive/datalake/mapme.protectedareas/output/matching/matching_frames/projects")

# drop BMZ Numbers before 2004 and BMZ number starting with 2098
T_year <- T_year[15:33]
bmz_list <- gsub("matching_frame_", "", T_year) %>% 
  gsub(".rds", "", .)
  
for (bmz in bmz_list) {  
custom_cem(bmz, cutoffs_travel_time = cutoffs_travel_time, cutoffs_loss_tn = cutoffs_loss_tn,cutoffs_elevation = cutoffs_elevation, cutoffs_tri = cutoffs_tri, cutoffs_soil = cutoffs_soil)}
```
 


## Timer

```{r timer}
# Timer
end_time <- Sys.time()

duration <- difftime(end_time, start_time, units='mins')
duration
```

